{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "import copy\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import ipynb.fs.full.misc as misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none(parent, rate):\n",
    "    return parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_r(parent, target_divergence):\n",
    "    assert len(parent.observation_buffer) == len(parent.action_buffer)\n",
    "\n",
    "    observations = np.asarray(parent.observation_buffer, dtype=np.float32)\n",
    "    actions = np.asarray(parent.action_buffer, dtype=np.float32)\n",
    "\n",
    "    observations, actions = skshuffle(observations, actions, n_samples=min(100, observations.shape[0]))\n",
    "\n",
    "    noise = np.random.normal(0.0, 1.0, parent.size)\n",
    "    scale = 1.0\n",
    "\n",
    "    divergence = 100500.\n",
    "\n",
    "    net = parent.net\n",
    "\n",
    "    i = 0\n",
    "    new_dna = None\n",
    "    while divergence > target_divergence and i < 10:\n",
    "        new_dna = parent.dna + noise * scale\n",
    "        net.inject_parameters(new_dna)\n",
    "\n",
    "        inp = misc.batch_to_torch(observations, cuda=net.is_cuda())\n",
    "        res = net(inp)\n",
    "        out = misc.batch_from_torch(res, cuda=net.is_cuda())\n",
    "\n",
    "        divergence = misc.squared_loss(out, actions)\n",
    "        scale /= 2.0\n",
    "        i += 1\n",
    "\n",
    "    return parent.produce_offspring(new_dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_g_sum(parent, rate):\n",
    "    assert len(parent.observation_buffer) == len(parent.action_buffer)\n",
    "    net = parent.net\n",
    "    \n",
    "    # prepare actions and observations\n",
    "    observations = np.asarray(parent.observation_buffer, dtype=np.float32)\n",
    "    actions = np.asarray(parent.action_buffer, dtype=np.float32)    \n",
    "    observations, actions = skshuffle(observations, actions, n_samples=min(100, observations.shape[0]))\n",
    "    observations = misc.batch_to_torch(observations, cuda=net.is_cuda())\n",
    "    \n",
    "    tot_size = net.parameters_count\n",
    "    is_cuda = net.is_cuda()\n",
    "    \n",
    "    # Jacobian is of outputs to parameters\n",
    "    jacobian = torch.zeros(net.num_outputs, tot_size)\n",
    "    \n",
    "    # Save the behaviour of parent's net, it will be used later\n",
    "    old_policy = net(observations)\n",
    "    \n",
    "    # Initialize Gradients with zeroes\n",
    "    grad_output = torch.zeros(*old_policy.size())\n",
    "    if is_cuda:\n",
    "        grad_output = grad_output.cuda()\n",
    "\n",
    "    # Set exclusively every output to 1 and compute gradient of parameters    \n",
    "    for i in range(net.num_outputs):\n",
    "        net.zero_grad()\n",
    "        grad_output.zero_()\n",
    "        grad_output[:, i] = 1.0\n",
    "\n",
    "        # do a backward pass to get partial derivatives for every output\n",
    "        old_policy.backward(grad_output, retain_graph=True)\n",
    "        \n",
    "        # for every output, put gradients computed into jacobian\n",
    "        # it will be in the same shape as parameters \n",
    "        jacobian[i] = torch.from_numpy(net.extract_grad())\n",
    "\n",
    "    # Calculate summed gradients sensitivity\n",
    "    # Sum axis is the axis of outputs\n",
    "    scaling = torch.sqrt((jacobian ** 2).sum(0))\n",
    "\n",
    "    # Prevent something bad that can possibly happen due to numeric issues\n",
    "    scaling[scaling == 0] = 1.0\n",
    "    scaling[scaling < 0.01] = 0.01\n",
    "\n",
    "    # Get random direction of perturbation\n",
    "    noise = np.random.normal(0.0, rate, parent.size)\n",
    "    \n",
    "    # Modify the direction with respect to sensitivity\n",
    "    delta = noise / scaling\n",
    "    \n",
    "    # Again, prevent windups\n",
    "    final_delta = np.clip(delta, -0.2, 0.2)\n",
    "\n",
    "    # Take a step in the direction of scaled perturbation\n",
    "    params = net.extract_parameters() + final_delta\n",
    "\n",
    "    child = parent.produce_offspring(params)\n",
    "    return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_g_so(parent, rate):\n",
    "    assert len(parent.observation_buffer) == len(parent.action_buffer)\n",
    "    net = parent.net\n",
    "    is_cuda = net.is_cuda()\n",
    "    \n",
    "    observations = np.asarray(parent.observation_buffer, dtype=np.float32)\n",
    "    actions = np.asarray(parent.action_buffer, dtype=np.float32)\n",
    "\n",
    "    observations, actions = skshuffle(observations, actions, n_samples=min(256, observations.shape[0]))\n",
    "\n",
    "    observations = misc.batch_to_torch(observations, cuda=net.is_cuda())\n",
    "\n",
    "    old_policy = net(observations)\n",
    "\n",
    "    np_copy = np.array(old_policy.data.cpu().numpy(), dtype=np.float32)\n",
    "    _old_policy_cached = Variable(torch.from_numpy(np_copy), requires_grad=False)\n",
    "    \n",
    "    if is_cuda:\n",
    "        _old_policy_cached = _old_policy_cached.cuda()\n",
    "\n",
    "    # loss = a measure of squared divergence from the old policy\n",
    "    loss = ((old_policy - _old_policy_cached) ** 2).sum(1).mean(0)\n",
    "\n",
    "    # take a first derivative\n",
    "    # compute gradient of loss w.r.t net parameters\n",
    "    loss_gradient = torch.autograd.grad(loss, net.parameters(), create_graph=True)\n",
    "    flat_gradient = torch.cat([grads.view(-1) for grads in loss_gradient])\n",
    "    if is_cuda:\n",
    "        flat_gradient = flat_gradient.cuda()\n",
    "\n",
    "    # choose a perturbation direction\n",
    "    delta = np.random.normal(0.0, rate, parent.size)\n",
    "    # normalize it\n",
    "    direction = (delta / np.sqrt((delta ** 2).sum()))\n",
    "    \n",
    "    #convert to pytorch\n",
    "    direction = np.asarray(direction, dtype=np.float32)\n",
    "    direction_t = Variable(torch.from_numpy(direction), requires_grad=False)\n",
    "    if is_cuda:\n",
    "        direction_t = direction_t.cuda()\n",
    "\n",
    "    # calculate second derivative along perturbation direction\n",
    "    grad_v_prod = (flat_gradient * direction_t).sum()\n",
    "    second_deriv = torch.autograd.grad(grad_v_prod, net.parameters())\n",
    "\n",
    "    # extract a contiguous version of the second derivative\n",
    "    sensitivity = torch.cat([g.contiguous().view(-1) for g in second_deriv])\n",
    "\n",
    "    # return our re-scaling based on second order sensitivity\n",
    "    scaling = torch.sqrt(torch.abs(sensitivity).data)\n",
    "\n",
    "    scaling[scaling == 0] = 1.0\n",
    "    scaling[scaling < 0.01] = 0.01\n",
    "\n",
    "    delta /= scaling\n",
    "    final_delta = np.clip(delta, -1.0, 1.0)\n",
    "\n",
    "    params = net.extract_parameters() + final_delta\n",
    "\n",
    "    return parent.produce_offspring(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_g_sum_r(parent, target_divergence):\n",
    "    assert len(parent.observation_buffer) == len(parent.action_buffer)\n",
    "    net = parent.net\n",
    "\n",
    "    observations = np.asarray(parent.observation_buffer, dtype=np.float32)\n",
    "    actions = np.asarray(parent.action_buffer, dtype=np.float32)\n",
    "\n",
    "    observations, actions = skshuffle(observations, actions, n_samples=min(100, observations.shape[0]))\n",
    "\n",
    "    n_elements = -min(100, observations.shape[0])\n",
    "    observations = observations[n_elements:]\n",
    "    actions = actions[n_elements:]\n",
    "\n",
    "    observations = misc.batch_to_torch(observations, cuda=net.is_cuda())\n",
    "\n",
    "    tot_size = net.parameters_count\n",
    "    jacobian = torch.zeros(net.num_outputs, tot_size)\n",
    "    old_policy = net(observations)\n",
    "    grad_output = torch.zeros(*old_policy.size())\n",
    "\n",
    "    if net.is_cuda():\n",
    "        grad_output = grad_output.cuda()\n",
    "\n",
    "    # do a backward pass for each output\n",
    "    for i in range(net.num_outputs):\n",
    "        net.zero_grad()\n",
    "        grad_output.zero_()\n",
    "        grad_output[:, i] = 1.0\n",
    "\n",
    "        old_policy.backward(grad_output, retain_graph=True)\n",
    "        jacobian[i] = torch.from_numpy(net.extract_grad())\n",
    "\n",
    "    # summed gradients sensitivity\n",
    "    scaling = torch.sqrt((jacobian ** 2).sum(0))\n",
    "\n",
    "    scaling[scaling == 0] = 1.0\n",
    "    scaling[scaling < 0.01] = 0.01\n",
    "\n",
    "    noise = np.random.normal(0.0, 1.0, parent.size)\n",
    "    delta = noise / scaling\n",
    "    delta = np.clip(delta, -1.0, 1.0)\n",
    "    divergence = 100500.\n",
    "\n",
    "    net = copy.deepcopy(parent.net)\n",
    "\n",
    "    i = 0\n",
    "    new_dna = None\n",
    "    scale = 1.0\n",
    "    while divergence > target_divergence and i < 10:\n",
    "        new_dna = parent.dna + delta * scale\n",
    "        net.inject_parameters(new_dna)\n",
    "\n",
    "        res = net(observations)\n",
    "        out = misc.batch_from_torch(res, cuda=net.is_cuda())\n",
    "\n",
    "        divergence = misc.squared_loss(out, actions)\n",
    "        \n",
    "        scale /= 2.0\n",
    "        i += 1\n",
    "\n",
    "    return parent.produce_offspring(new_dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(parent, variance):\n",
    "    noise = np.random.normal(0.0, variance, parent.size)\n",
    "    dna = parent.get_dna() + noise\n",
    "    \n",
    "    return parent.produce_offspring(dna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
